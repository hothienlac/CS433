{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Explained\n",
    "\n",
    "# Module 4 - Lab - Introduction to Regularization for Deep Neural Nets     \n",
    "\n",
    "\n",
    "\n",
    "This lesson will introduce you to the principles of regularization required to successfully train deep neural networks. In this lesson you will:\n",
    "\n",
    "1. Understand the need for regularization of complex machine learning models, particularly deep NNs. \n",
    "2. Know how to apply constraint-based regularization using the L1 and L2 norms.\n",
    "3. Understand and apply the concept of data augmentation. \n",
    "4. Know how to apply dropout regularization. \n",
    "5. Understand and apply early stopping. \n",
    "6. Understand the advantages of various regularization methods and know when how to apply them in combination. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.0 Why do we need regularization for deep learning?\n",
    "\n",
    "Deep learning models have a great many parameters (weights) which must be fit. This situation arises from the wide and deep architectures that are required to achieve significant **model capacity** for representing complex functions. The core issue is that over-fit models will simply learn the training data and **over-fit models do not generalize**. Therefore, regularization methods are required in order to prevent over-fitting.\n",
    "\n",
    "In particular, we can point to three interrelated problems with training deep neural networks:\n",
    "\n",
    "1. Neural network models have large numbers of parameters (weights). With any finite size data set, there is likely to be a low ratio of cases per parameter or low ratio of cases to features. \n",
    "2. As a result of the large numbers of parameters, neural networks are susceptible to noise in the training data. Neural networks are generally considered less robust to noise than shallow machine learning methods. \n",
    "3. Presumably as a result of the model complexity, neural networks often return unexpected predictions for data cases outside the training data domain. This property has been referred to as **brittleness**. Brittleness has proven to be a serious problem in some production systems. \n",
    "\n",
    "The regularization methods presented here will limit these effects. However, there is no 'silver bullet'! Neural networks are hard to train under the best of circumstances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bias-variance trade-off\n",
    "\n",
    "To better understand this trade-off let's decompose mean square error for a model as follows:\n",
    "\n",
    "$$\\Delta y = E \\big[ Y - \\hat{f}(X) \\big]$$\n",
    "\n",
    "Where,     \n",
    "$Y = $ the label vector.  \n",
    "$X = $ the feature matrix.   \n",
    "$\\hat{f}(x) = $ the trained model.   \n",
    "\n",
    "Expanding this relation gives us:\n",
    "\n",
    "$$\\Delta y = \\big( E[ \\hat{f}(X)] - \\hat{f}(X) \\big)^2 + E \\big[ ( \\hat{f}(X) - E[ \\hat{f}(X)])^2 \\big] + \\sigma^2\\\\\n",
    "\\Delta y = Bias^2 + Variance + Irreducible\\ Error$$\n",
    "\n",
    "\n",
    "Regularization will reduce variance, but increase bias. Regularization parameters must be chosen to minimize $\\Delta x$. In many cases, this will prove challenging. \n",
    "\n",
    "Notice that the **irreducible error** is the limit of model accuracy. Even if we had a perfect model with no bias or variance, the irreducible error is inherent in the data and problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Demonstration of over-parameterization\n",
    "\n",
    "Let's try a simple example. We will construct a regression models with different numbers of parameters and therefore different model capacities. \n",
    "\n",
    "As a first step, we will create a simple single regression model of some synthetic data. The code in the cell below creates data computed from as a straight line, but with considerable Normally distributed random noise. A plot is then created of the result. Execute this code and examine the resulting plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import normal, seed\n",
    "import sklearn.linear_model as slm\n",
    "from sklearn.preprocessing import scale\n",
    "import sklearn.model_selection as ms\n",
    "from math import sqrt\n",
    "import keras\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dropout, LeakyReLU\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "seed(34567)\n",
    "x = np.arange(start = 0.0, stop = 10.0, step = 0.25) \n",
    "y = np.add(x, normal(scale = 2.0, size = x.shape[0]))\n",
    "\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these data points fall approximately on a straight line, but with significant deviations. \n",
    "\n",
    "Next, you will compute a simple single regression model. This model has an intercept term and a single slope parameter. The code in the cell below splits the data into randomly selected training and testing subsets. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = range(len(x))\n",
    "seed(9988)\n",
    "indx = ms.train_test_split(indx, test_size = 20)\n",
    "x_train = np.ravel(x[indx[0]])\n",
    "y_train = np.ravel(y[indx[0]])\n",
    "x_test = np.ravel(x[indx[1]])\n",
    "y_test = np.ravel(y[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the linear model in `sklearn.linear_model` package to create a single regression model for these data. The code in the cell below does just this, prints the single model coefficient, and plots the result. Execute this code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_reg(x, y_score, y):\n",
    "    ax = plt.figure(figsize=(6, 6)).gca() # define axis\n",
    "    \n",
    "    ## Get the data in plot order\n",
    "    xy = sorted(zip(x,y_score))\n",
    "    x = [x for x, _ in xy]\n",
    "    y_score = [y for _, y in xy]\n",
    "\n",
    "    ## Plot the result\n",
    "    plt.plot(x, y_score, c = 'red')\n",
    "    plt.scatter(x, y)\n",
    "    plt.title('Predicted line with test data')\n",
    "\n",
    "def reg_model(x, y):\n",
    "    mod = slm.LinearRegression()\n",
    "    x_scale = scale(x)  # .reshape(-1, 1)\n",
    "    mod.fit(x_scale, y)\n",
    "    print(mod.coef_)\n",
    "    return mod, x_scale, mod.predict(x_scale)\n",
    "\n",
    "mod, x_scale, y_hat = reg_model(x_train.reshape(-1, 1), y_train)\n",
    "\n",
    "plot_reg(x_scale, y_hat, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice that the single coefficient (slope) seems reasonable, given the standardization of the training data. Visually, the fit to the training data also looks reasonable. \n",
    "\n",
    "We should also test the fit to some test data. The code in the cell does just this and returns the RMS error. execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def test_mod(x,y, mod):\n",
    "    x_scale = scale(x)\n",
    "    y_score = mod.predict(x_scale)\n",
    "    plot_reg(x_scale, y_score, y)\n",
    "    return np.std(y_score - y)\n",
    "\n",
    "test_mod(x_test.reshape(-1, 1), y_test, mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these results look reasonable. The RMSE is relatively small given the significant dispersion in these data. \n",
    "\n",
    "Now, try a model with significantly higher capacity. In this case we compute new features for a 9th order polynomial model. Using this new set of features a regression model is trained and a summary displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed(2233)\n",
    "x_power = np.power(x_train.reshape(-1, 1), range(1,10))\n",
    "x_scale = scale(x_power)\n",
    "\n",
    "mod_power = slm.LinearRegression()\n",
    "mod_power.fit(x_scale, y_train)\n",
    "y_hat_power = mod_power.predict(x_scale)\n",
    "\n",
    "plot_reg(x_scale[:,0], y_hat_power, y_train)\n",
    "\n",
    "print(mod_power.coef_)\n",
    "print(np.std(y_hat_power - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following, indicating the model is quite over-fit. \n",
    "- There is a wide range of coefficient values across 7 orders of magnitude. This situation is in contrast to the coefficient of the single regression model which had a reasonable single digit value.\n",
    "- The graph of the fitted model shows highly complex behavior. In reality, this behavior indicates the model is 'learning the data'.  \n",
    "\n",
    "Now, we will try to test the model with the held-back test data. The code in the cell below creates the same features and applies the `predict` method to the model using these test features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test_scale = scale(x_test.reshape(-1, 1)) # Prescale to prevent numerical overflow. \n",
    "x_test_power = np.power(x_test_scale, range(1,10))\n",
    "x_scale_test = scale(x_test_power)\n",
    "\n",
    "y_hat_power = mod_power.predict(x_scale_test)\n",
    "\n",
    "plot_reg(x_scale_test[:,0], y_hat_power, y_test)\n",
    "\n",
    "print(np.std(y_hat_power - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly a terrible fit! The RMSE is enormous and the curve of predicted values bears little resemblance to the test values. Indeed, this is a common problem with over-fit models that the errors grow in very rapidly toward the edges of the training data domain. We can definitely state that this model **does not generalize**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 l2 regularization\n",
    "\n",
    "We will now explore one of the mostly widely used regularization methods, often referred to as l2 regularization. \n",
    "\n",
    "The same method goes by some other names, as it has been 'invented' several times. In particular, this method is known as, **Tikhonov regularization**, **l2 norm regularization**, **pre-whitening** in engineering, and for linear models **ridge regression**. In all likelihood the method was first developed by the Russian mathematician Andrey Tikhonov in the late 1940's. His work was not widely known in the West since his short book on the subject, [Solution of Ill-Posed Problems](https://www.researchgate.net/publication/44438630_Solutions_of_ill-posed_problems_Andrey_N_Tikhonov_and_Vasiliy_Y_Arsenin), was only published in English in 1977, about 30 years after it had appeared in Russian.\n",
    "\n",
    "![](img/Tikhonov_board.jpg)\n",
    "<center> **Figure 2.1   \n",
    "Commemorative plaque for Andrey Nikolayevich Tikhonov at Moscow State University**\n",
    "\n",
    "\n",
    "So, what is the basic idea? l2 regularization applies a **penalty** proportional to the **l2** or **Euclidean norm** of the model weights to the loss function. The total loss function then becomes:  \n",
    "\n",
    "$$J(W) = J_{MLE}(W) + \\lambda ||W||^2$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$$||W||^2 = \\big( w_1^2 + w_2^2 + \\ldots + w_n^2 \\big)^{\\frac{1}{2}} = \\Big( \\sum_{i=1}^n w_i^2 \\Big)^{\\frac{1}{2}}$$\n",
    "\n",
    "We call $||W||^2$ the l2 norm of the weights since we square the power of the weights, sum, and then take the square root, or $\\frac{1}{2}$ power. \n",
    "\n",
    "You can think of this penalty as constraining the 12 or Euclidean norm of the model weight vector. The value of the hyperparameter $\\lambda$ determines how much the norm of the coefficient vector constrains the solution. You can see a view of this geometric interpretation in Figure 2.2 below.  \n",
    "\n",
    "![](img/L2.jpg)\n",
    "<center> **Figure 2.2. Geometric view of l2 regularization**\n",
    "\n",
    "Notice that for a constant value of l2, the values of the model parameters $B1$ and $B2$ are related. For example, if $B1$ is maximized then $B2 \\sim 0$, or vice versa. It is important to note that l2 regularization is a **soft constraint**. Coefficients are driven close to, but likely not exactly to, zero.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Regularization for regression  \n",
    "\n",
    "Let's go back to the regression example. Recall that the 9th order polynomial regression model was massively over-fit. Can l2 regularization help this situation? We can create a model applying regularization and find out. \n",
    "\n",
    "The code in the cell below uses the `Ridge` model from `sklearn.linear_model`. The `Ridge` model has an argument `alpha` which corresponds to the regularization parameter, in the notation we have been using. Execute the code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod_L2 = slm.Ridge(alpha = 100.0)\n",
    "mod_L2.fit(x_scale, y_train)\n",
    "y_hat_L2 = mod_L2.predict(x_scale)\n",
    "\n",
    "print(np.std(y_hat_L2 - y_train))\n",
    "print(mod_L2.coef_)\n",
    "\n",
    "plot_reg(x_train, y_hat_L2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is quite different from the un-regularized one we trained previously. \n",
    "- The coefficients all have small values. Some of the coefficients are significantly less than 1. These small coefficients are a direct result of the l2 penalty.\n",
    "- The fitted curve looks rather reasonable given the noisy data.\n",
    "\n",
    "Now test the model on the test data. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat_L2 = mod_L2.predict(x_scale_test)\n",
    "\n",
    "plot_reg(x_scale_test[:,0], y_hat_L2, y_test)\n",
    "\n",
    "print(np.std(y_hat_L2 - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result looks a lot more reasonable. The RMSE is nearly the same as for the single feature regression example. Also, the predicted curve looks reasonable.\n",
    "\n",
    "In summary, we have seen that l2 regularization significantly improves the result for the 9th order polynomial regression. The coefficients are kept within a reasonable range and the predictions are much more reasonable than the unconstrained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 l2 regularization for deep learning models \n",
    "\n",
    "So, you may well wonder, how well l2 regularization applies to neural networks? Let's give it a try using the 9th order polynomial data.  \n",
    "\n",
    "The code in the cell below defines and fits the regression model with a single hidden layer with 128 units. No regularization is applied in this first model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(345)\n",
    "set_random_seed(4455)\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, activation = 'relu', input_shape = (9, )))\n",
    "nn.add(layers.Dense(1))\n",
    "nn.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 30, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                   verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model fit, let's have a look at the loss function vs. training epoch. Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    x = list(range(1, len(test_loss) + 1))\n",
    "    plt.plot(x, test_loss, color = 'red', label = 'Test loss')\n",
    "    plt.plot(x, train_loss, label = 'Train loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    \n",
    "plot_loss(history) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this model becomes overfit after 3 or 4 training epochs. \n",
    "\n",
    "Execute the code in the cell below to compute and plot predictions for the unconstrained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 4, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                   verbose = 0)\n",
    "predicted = nn.predict(x_scale_test)\n",
    "plot_reg(x_scale_test[:,0], predicted, y_test)\n",
    "print(np.std(predicted - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the high RMSE and the odd behavior of the predicted curve indicates that this model does not generalize well at all. Notice in particular, how the predicted curve moves away from the test data values on the right. \n",
    "\n",
    "Now, we will try to improve this result by applying l2 norm regularization to the neural network. The code in cell below adds l2 regularization to the model. Execute the code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(45678)\n",
    "set_random_seed(45546)\n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, activation = 'relu', input_shape = (9, ),\n",
    "                        kernel_regularizer=regularizers.l2(2.0)))\n",
    "nn.add(layers.Dense(1))\n",
    "nn.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 30, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  verbose = 0)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function is quite a bit different than for the unconstrained model. It is clear that regularization allows many more training epochs before over-fitting. \n",
    "\n",
    "But are the predictions any better? Execute the code in the cell below and find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 30, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  verbose = 0)\n",
    "predicted = nn.predict(x_scale_test)\n",
    "plot_reg(x_scale_test[:,0], predicted, y_test)\n",
    "print(np.std(predicted - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The l2 regularization has reduced the RMSE. Just as significantly, the pathological behavior of the predicted values on the right is reduced, but clearly not eliminated. The bias effect is also visible. Notice that the left part of the fitted curve is now shifted upwards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************\n",
    "**Exercise 1:** You have now tried l2 regularization with one choice of regularization hyperparameter, namely the regularization parameter. Finding a good choice for the regularization parameter can require some trial and error. The objective is to find a value that produces a minimum test error.\n",
    "\n",
    "In the code cells below, create models as follows: \n",
    "1. A regularization parameter of 20.0, using a `numpy.random.seed` of 9456 and `set_random_seed` for the TensorFlow backend of 55566\n",
    "2. A regularization parameter of 200.0, using a `numpy.random.seed` of 9566 and `set_random_seed` for the TensorFlow backend of 44223. \n",
    "\n",
    "Plot the loss history for both models. Make sure you give you models different names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(9456)\n",
    "set_random_seed(55566)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(9566)\n",
    "set_random_seed(44223)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in the cells below you will create code to compute and plot the predicted values from your model for the test data, along with the error metric. Include the test data values on your plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history20 = nn20.fit(x_scale, y_train, \n",
    "                  epochs = 30, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  verbose = 0)\n",
    "predicted20 = nn20.predict(x_scale_test)\n",
    "plot_reg(x_scale_test[:,0], predicted20, y_test)\n",
    "print(np.std(predicted20 - y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history200 = nn200.fit(x_scale, y_train, \n",
    "                  epochs = 30, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  verbose = 0)\n",
    "predicted200 = nn200.predict(x_scale_test)\n",
    "plot_reg(x_scale_test[:,0], predicted200, y_test)\n",
    "print(np.std(predicted200 - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compare the results for the three models with regularization hyperparameter values of 2.0, 20.0, and 200.0. Notice how the RMSE improves as the hyperparameter increases. Notice also, that the test loss for the highest hyperparameter value decreases most uniformly, indicating less over-fitting of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 l1 regularization\n",
    "\n",
    "We can also do regularization using other norms. The **l1 regularization** or **Lasso**  method limits the sum of the absolute values of the model coefficients. The l1 norm is sometime know as the **Manhattan norm**, since distance are measured as if you were traveling on a rectangular grid of streets. This is in contrast to the l2 norm that measures distance 'as the crow flies'. \n",
    "\n",
    "We can compute the l1 norm of the weights as follows:\n",
    "\n",
    "$$||W||^1 = \\big( |w_1| + |w_2| + \\ldots + |w_n| \\big) = \\Big( \\sum_{i=1}^n |w_i| \\Big)^1$$\n",
    "\n",
    "where $|x|$ is the absolute value of $x$. \n",
    "\n",
    "Notice that to compute the l1 norm, we raise the sum of the absolute values to the first power.\n",
    "\n",
    "As with l2 regularization, in l1 regularization  we use a penalty term of the l1 norm of the weights. A penalty multiplier, $\\alpha$, determines how much the norm of the coefficient vector constrains values of the weights. The complete loss function then becomes: \n",
    "\n",
    "$$J(W) = J_{MLE}(W) + \\alpha ||W||^1$$\n",
    "\n",
    "You can see a view of this geometric interpretation in Figure 3.1 below.  \n",
    "\n",
    "![](img/L1.jpg)\n",
    "<center> **Figure 3.1. Geometric view of L1 regularization**\n",
    "\n",
    "Notice that in Figure 3.1 if $B1 = 0$ then $B2$ has a value at the limit, or vice versa. In other words, using a l1 norm constraint forces some weight values to zero to allow other coefficients to take correct values. In this way, the l1 norm constraint **knocks out** some weights from the model altogether. In contrast to l2 regularization, l1 regularization will drive some coefficients to exactly zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 l1 regularization\n",
    "\n",
    "With these ideas in mind, let's apply l1 norm regularization to the 9th order polynomial regression problem. The code in cell below applies l1 regularized or Lasso regularization to the linear regression problem. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_L1 = slm.Lasso(alpha = 2.0, max_iter=100000)\n",
    "mod_L1.fit(x_scale, y_train)\n",
    "y_hat_L1 = mod_L1.predict(x_scale)\n",
    "\n",
    "print(np.std(y_hat_L1 - y_test))\n",
    "print(mod_L1.coef_)\n",
    "\n",
    "plot_reg(x_train, y_hat_L1, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about the results of this l1 regularized regression:\n",
    "- Many of the coefficients are 0, as expected.\n",
    "- The fitted curve looks reasonable. \n",
    "\n",
    "Now, execute the code in the cell below and examine the prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_L1 = mod_L1.predict(x_scale_test)\n",
    "\n",
    "plot_reg(x_scale_test[:,0], y_hat_L1, y_test)\n",
    "\n",
    "print(np.std(y_hat_L1 - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE has been reduced considerably, and is less than for l2 regularization regression. The plot of predicted values looks similar to the single regression model, but with some bias. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Neural network with l1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try l1 regularization with a neural network. The code in the cell below defines, fits and plots a single layer neural network using l1 regularization. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, activation = 'relu', input_shape = (9, ),\n",
    "                        kernel_regularizer=regularizers.l1(10.0)))\n",
    "nn.add(layers.Dense(1))\n",
    "nn.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 100, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  verbose = 0)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the l1 regularization the training loss does not exhibit signs of over-fitting for quite a few epochs. \n",
    "\n",
    "Next, excute the code in the cell below to compute and display predicted values from the trained network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 40, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  verbose = 0)\n",
    "predicted = nn.predict(x_scale_test)\n",
    "plot_reg(x_scale_test[:,0], predicted, y_test)\n",
    "print(np.std(predicted - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are a definite improvement. The RMSE is similar to that produced by the l2 regularization neural network. Further, the fitting curve shows similar behavior and bias. This bias is the result of the regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Early stopping\n",
    "\n",
    "Early stopping is conceptually simple. Early stopping terminates the training of the neural network model at an epoch before it becomes terribly over-fit. That's it! That is the idea of early stopping.\n",
    "\n",
    "In fact, we have already been using early stopping as we create and test the foregoing regularized models. The question here is, how do we automate this process? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Early stopping algorithm\n",
    "\n",
    "The early stopping algorithm simple. This pseudo code shows the basic loop for early stopping on first epoch with a lower performance metric, which is executed after the first training epoch of the model. \n",
    "\n",
    "`Do while TRUE:  \n",
    "    store current model parameters   \n",
    "    update model for epoch  \n",
    "    if(performance_for_epoch < stored_performance_metric)  \n",
    "        return stored_model  \n",
    "    else  \n",
    "        stored_performance_metric = performance_for_epoch   \n",
    "        store_model = model  \n",
    "`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 How does early stopping work?\n",
    "\n",
    "Early stopping terminates model learning before over-fitting occurs. But how can we interpret this action in terms of the loss function $J(W)_{MLE}$? Figure 4.1 below provides some insight.   \n",
    "\n",
    "![](img/EarlyStopping.JPG)    \n",
    "<center>**Figure 4.1 Effect of early stopping on $J(W)_{MLE}$**</center>\n",
    "\n",
    "On the left side of the diagram you can see contours of the weight norm. On the right are contours  Early stopping terminates training at some model weight norm $||W||^2$. Ideally this is at the point where the training of $J(W)_{MLE}$ starts to over-fit. Thus, we can think of early stopping as analogous to l2 norm regularization where we write the loss function as:\n",
    "\n",
    "$$argmin_W J(W) = J(W)_{MLE} + \\alpha ||W||^2$$\n",
    "\n",
    "where,\n",
    "\n",
    "$\\alpha = $ a regularization parameter controlling the stopping point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Early stopping example\n",
    "\n",
    "Manually applying early stopping is both computationally inefficient and rather tedious. Fortunately, Keras has a build in capability that allows automation. \n",
    "\n",
    "To implement this early stopping we need to define 2 Keras **callbacks**. Two such callbacks are required:\n",
    "1. The first callback, **EarlyStopping**, is for the early stopping method.\n",
    "2. The second call back **checkpoints** or saves the current model. \n",
    "\n",
    "These callbacks are defined in the form of a **callbacks list**. \n",
    "\n",
    "Notice that the model defined includes l2 regularization. Thus, this model should replicate the performance observed with manual early stopping. To see how this works, examine and then execute the code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First define and compile a model. \n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, activation = 'relu', input_shape = (9, ),\n",
    "                        kernel_regularizer=regularizers.l2(1.0)))\n",
    "nn.add(layers.Dense(1))\n",
    "\n",
    "nn.compile(optimizer = 'RMSprop', loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "## Define the callback list\n",
    "filepath = 'my_model_file.hdf5' # define where the model is saved\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss', # Use loss to monitor the model\n",
    "        patience = 1 # Stop after one step with lower accuracy\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath, # file where the checkpoint is saved\n",
    "        monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "        save_best_only = True # Only save model if it is the best\n",
    "    )\n",
    "]\n",
    "\n",
    "## Now fit the model\n",
    "nr.seed(5566)\n",
    "set_random_seed(6767)\n",
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 40, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 0)\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the behavior of the loss with training epoch is behaving as with l2 regularization alone. Notice that the training has been automatically terminated at the point the loss function is at its optimum. \n",
    "\n",
    "Let's also have a look at the accuracy vs. epoch. Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    train_acc = history.history['mean_absolute_error']\n",
    "    test_acc = history.history['val_mean_absolute_error']\n",
    "    x = list(range(1, len(test_acc) + 1))\n",
    "    plt.plot(x, test_acc, color = 'red', label = 'Test error rate')\n",
    "    plt.plot(x, train_acc, label = 'Train error rate')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error rate')\n",
    "    plt.title('Error Rate vs. Epoch')  \n",
    "    \n",
    "plot_accuracy(history)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve of test accuracy is consistent with the test loss.\n",
    "\n",
    "The code in the cell below retrieves the best model (by our stopping criteria) from storage, computes predictions and displays the result. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(filepath)\n",
    "predictions = best_model.predict(x_scale_test)\n",
    "plot_reg(x_scale_test[:,0], predictions, y_test)\n",
    "print(np.std(predictions - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these results are similar, but a bit worse, than those obtained while manually stopping the training of the l2 regularized neural network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Dropout regularization\n",
    "\n",
    "All of the regularization methods we have discussed so far, originated long before the current deep neural network era. We will now look at the **dropout regularization** method. Of all widely used regularization methods, dropout is one of the few specifically developed for neural networks. The seminal paper, [Dropout: A Simple Way to Prevent Neural Networks from\n",
    "Overfitting by Srivastava et. al](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf), 2014, is quite readable and provides a lot more detail than is presented here.\n",
    "\n",
    "We have already seen how l1 norm regularization knocks out some model weights. The dropout method regularizes neural networks by creating an **ensemble** of networks with some fraction $p \\lt 1.0$ of the hidden units removed. Ensemble methods are know to be strong regularizers and produce superior results by combining the learning of multiple **weak learners**. \n",
    "\n",
    "The dropout method is somewhat different from other ensemble methods, such as bagging. This reweighting scheme has several advantages:\n",
    "- The model weights for the resulting networks are reweighted by the probabilities that they are sampled in the ensemble. \n",
    "- The memory required to train the model is simply $O(n)$, where n is the number of weights. A bagged model requires  $O(M*n)$, where $M$ is the number of models in the ensemble.\n",
    "- When making predictions in production only one model is used. Whereas, the predictions for each model in the bag must be computed for bagging. \n",
    "\n",
    "To understand this method, let's recall the basic model for the output of a lth layer in a fully connected network:\n",
    "\n",
    "$$z^{(l+1)}_i = w^{(l+1)}_i \\cdot h^{(l)} + b^{(l+1)}_i\\\\\n",
    "h^{(l+1)}_i = \\sigma(z^{(l+1)}_i)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\sigma = $ the activation function. \n",
    "\n",
    "Now, we need to sample the hidden units with probability $p$, in which case we can write:\n",
    "\n",
    "$$r^{(l)}_i \\sim Bernoulli(p)\\\\\n",
    "\\tilde{h}^{(l)}_i = r^{(l)}_i * y^{(l)}\\\\\n",
    "z^{(l+1)}_i = w^{(l+1)}_i \\cdot \\tilde{h}^{(l)}_i + b^{(l+1)}_i\\\\\n",
    "h^{(l+1)}_i = \\sigma(z^{(l+1)}_i)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$r^{(l)}_i =$dropout vector with values $\\{0,1\\}$.\n",
    "\n",
    "To get a feel for what this means in practice examine Figure 5.1. This figure shows a fully connected network with 4 hidden units and a dropout probability $p = 0.5$. \n",
    "\n",
    "![](img/DropoutExample.JPG)\n",
    "![](img/DropoutExample2.JPG)\n",
    "\n",
    "<center>**Figure 5.1   \n",
    "Possible dropouts for a simple fully connected network with p = 0.5**</center>\n",
    "\n",
    "Examine Figure 5.1 and notice the following:\n",
    "\n",
    "- There are 6 ways to achieve dropout with exactly 1/2 the units as shown. \n",
    "- No units might dropout with probability $p^4$. \n",
    "- A single unit might drop out with probability $p^3 (1-p)$. \n",
    "- All units might drop out with probability $(1-p)^4$. This case is not admissible so should not be sampled. \n",
    "\n",
    "In fact there are $n^2$ possible dropout patterns for a hidden layer with n units. This scaling quickly leads to a problem. For any realistic size network, it is not possible to fully sample all of the possibilities. Instead, we need to use some kind of approximation with a reasonable number of samples. \n",
    "\n",
    "Ideally, we want a model that gives us the posterior probability of $y$, the output, given $x$ the input which we can write $p(y\\ |\\ x)$. If we had infinite computing resources we could Monte Carlo sample this distribution for our neural network. This ideal reference neural network is known as **Bayesian network**. Clearly, for large scale networks it is not possible to compute this result.   \n",
    "\n",
    "We have to settle for a sampled result. We reweight by the probability that a sample is created. Continuing with the notation we used before we can write:\n",
    "\n",
    "$$p(y\\ |\\ x) \\sim \\sum_r p(r) p(y\\ |\\ x, r)$$\n",
    "\n",
    "where,\n",
    "\n",
    "$r = $ the Bernoulli sampled mask vector. \n",
    "\n",
    "Given enough samples the approximation above will converge to the desired probability distribution. However, in practice it has been found that the **geometric mean** of the ensemble converges faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Computing a neural network with dropout regularization\n",
    "\n",
    "With a bit of theory in mind, we will now apply dropout regularization to training a neural network. The code in the cell below defines a neural network with a dropout layer with $p =0.5$. The rest of this network is identical to other networks we have been working with. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## First define and compile a model with a dropout layer. \n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, activation = 'relu', input_shape = (9, )))\n",
    "nn.add(Dropout(rate = 0.5)) # Use 50% dropout on this model\n",
    "nn.add(layers.Dense(1))\n",
    "nn.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "## Now fit the model\n",
    "nr.seed(1144)\n",
    "set_random_seed(6723)\n",
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 40, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 0)\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The familiar loss plot looks a bit different here. Notice the kinks in the training loss curve. This is likely a result of the dropout sampling. \n",
    "\n",
    "Execute the code in the cell below, and examine the accuracy vs. epoch curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of the training accuracy curve has a similar appearance to the loss curve in terms of the jagged appearance. \n",
    "\n",
    "Execute the code in the cell below examine the prediction results for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(filepath)\n",
    "predictions = best_model.predict(x_scale_test)\n",
    "plot_reg(x_scale_test[:,0], predictions, y_test)\n",
    "print(np.std(predictions - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results appear similar to those obtained with other regularization methods for neural networks on this problem, particularly, early stopping. While the dropout method is an effective regularizer it is no 'silver bullet'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Batch Normalization\n",
    "\n",
    "It is often the case that the distribution of output values of some hidden layers changes . The result is that propagated gradients can become near zero, significantly slowing convergence in many cases. We will discuss this **vanishing gradient problem** in another lesson.  \n",
    "\n",
    "In 2015, [Sergey and Szegedy](https://arxiv.org/pdf/1502.03167.pdf) introduced a solution to this problem with their paper **Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**. The basic idea is simple. A batch normalization layer maintains an exponential moving average estimate of the mean and variance of the outputs of layer. These values are used to normalize the output values of that layer. In other words, the batch normalization layer ensures the distribution of the output values are constant. \n",
    "\n",
    "Let's try an example. The simple neural network model defined in the code cell below includes a batch normalization layer. Also notice that to improve convergence the early stopping has been modified to have a patience of 3. Execute this code.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use patience of 3\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss', # Use loss to monitor the model\n",
    "        patience = 3 # Stop after three steps with lower accuracy\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath, # file where the checkpoint is saved\n",
    "        monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "        save_best_only = True # Only save model if it is the best\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "## Now, define an NN model using batch normalization. \n",
    "## First define and compile a model with a batch normalization layer. \n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, input_shape = (9, ), activation = 'relu'))\n",
    "nn.add(BatchNormalization(momentum = 0.99))\n",
    "nn.add(layers.Dense(1))\n",
    "## Define the optimizer and compile\n",
    "optm = keras.optimizers.rmsprop(lr=1.0)\n",
    "nn.compile(optimizer = optm, loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "## Now fit the model\n",
    "nr.seed(345)\n",
    "set_random_seed(4532)\n",
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 100, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 0)\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreases rapidly and then remains in a narrow range thereafter. It appears that convergence is quite rapid.\n",
    "\n",
    "How does the accuracy evolve with the training episodes? Execute the code in the cell below to display the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy curve is rather unusual. It seems to reflect the simple regularization being used. \n",
    "\n",
    "Finally, execute the code in the cell below to evaluate the predictions made with this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(filepath)\n",
    "predictions = best_model.predict(x_scale_test)\n",
    "plot_reg(x_scale_test[:,0], predictions, y_test)\n",
    "print(np.std(predictions - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit to the test data look fairly good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Using multiple regularization methods\n",
    "\n",
    "**Exercise 2:** In many cases more than one regularization method is applied. We have already applied early stopping with other regularization methods. In this exercise you will create a neural network work using four regularization methods at once:\n",
    "- l2 regularization\n",
    "- l1 regularization\n",
    "- Dropout\n",
    "- Early stopping \n",
    "\n",
    "In the cell below create code for a neural network using the above regularization methods. Your code should include the following:\n",
    "\n",
    "1. Set a `numpy.random` seed of 242244 and a `set_random_seed` for the TensorFlow backend of 4356.\n",
    "2. Define a call back list with `EarlyStopping` with monitor set to `val_loss` and patience set to 4, and the `ModelCheckpoint` with monitor set to `val_loss`\n",
    "3. A fully connected layer with 128 units and ReLU activation. Include l1 and l2 regulaization using the `l1_l2` function with the regularization parameter set to 50.0.\n",
    "4. A dropout layer with regularization parameter set to 0.5.\n",
    "5. Fit your model for 120 epochs, with batch size of 10, using the already defined callback list\n",
    "\n",
    "Fit you neural network model, saving the results to a history object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(242244)\n",
    "set_random_seed(4346)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below create and execute the code to plot the loss history for both training and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the outcome\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below create and execute the code to plot the accuracy history for both training and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in the cells below you will create code to compute and plot the predicted values from your model for the test data, along with the error metric. Include the test data values on your plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(filepath)\n",
    "predictions = best_model.predict(x_scale_test)\n",
    "plot_reg(x_scale_test[:,0], predictions, y_test)\n",
    "print(np.std(predictions - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these results compare to using single regularization methods. Has there been any improvement in accuracy? What about the bias in the fitted curve which is quite noticeable when some of the single regularization methods are used? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
